# Project: Web Data Pipeline

The main goal was to pick a data set and produce an exploratory data analysis, covering steps of acquisition, wrangling, analysis, and reporting. 

# Chosen Data Set: [SF Bay Area Bike Share](https://www.kaggle.com/benhamner/sf-bay-area-bike-share)
Additionally the data set [Historical Hourly Weather Data 2012-2017](https://www.kaggle.com/selfishgene/historical-hourly-weather-data) was used to provide more weather information. 

## Deliverables

### Code
[Jupyter Project](https://github.com/vmcarva/python/blob/master/SF%20Bay%20Bike%20Share%20Project%20-%20Exploratory%20Data%20Analysis/Bike%20Share%20Project.ipynb)

### Presentation
[Main Insights](https://github.com/vmcarva/python/blob/master/SF%20Bay%20Bike%20Share%20Project%20-%20Exploratory%20Data%20Analysis/SF%20Bay%20Area%20Bike%20Share.pdf)

### Interactive HTML Files
- [heatMap](https://github.com/vmcarva/python/blob/master/SF%20Bay%20Bike%20Share%20Project%20-%20Exploratory%20Data%20Analysis/heatmap.html)
- [Markers](https://github.com/vmcarva/python/blob/master/SF%20Bay%20Bike%20Share%20Project%20-%20Exploratory%20Data%20Analysis/marker.html)
- [heatMapTime](https://github.com/vmcarva/python/blob/master/SF%20Bay%20Bike%20Share%20Project%20-%20Exploratory%20Data%20Analysis/heatMapTime.html)
- [Simple Trip Flow](https://github.com/vmcarva/python/blob/master/SF%20Bay%20Bike%20Share%20Project%20-%20Exploratory%20Data%20Analysis/trip_flow_simple.html)
- [Trip Flow with Intensity](https://github.com/vmcarva/python/blob/master/SF%20Bay%20Bike%20Share%20Project%20-%20Exploratory%20Data%20Analysis/trip_flow_wintensity.html)

## Technical Requirements

The basic technical requirements for this project are as follows:

* Each data pipeline stage should be covered: acquisition, wrangling, analysis, and reporting.
* There should be some data set that gets imported and some result that gets exported.
* Your code should be saved in a Python executable file or Jupyter.

## How Did I Get Started?

* **Find a data set to process** - You can use a dataset you are familiar with like [Sharks](https://www.kaggle.com/teajay/global-shark-attacks/version/1) or find another. A great place to start looking would be [Awesome Public Data Sets](https://github.com/awesomedata/awesome-public-datasets) and [Kaggle Data Sets](https://www.kaggle.com/datasets).
* **Examine the data and come up with a deliverable** before diving in and applying any methods to it.
* **Break the project down into different steps** - leverage the stages of the data pipeline covered in the pipelines lesson and answer the appropriate questions for each stage.

